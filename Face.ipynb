{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load('./10_classes_embeddings.npz')\n",
    "data_alex = np.load('./Alex_emd.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_trainX , trainy_n , emb_testX , testy_n = data['arr_0'],data['arr_1'],data['arr_2'],data['arr_3']\n",
    "emb_trainX_alex , trainy_alex , emb_testX_alex , testy_alex = data_alex['arr_0'],data_alex['arr_1'],data_alex['arr_2'],data_alex['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_face(mainData,newData):\n",
    "    embed_con = np.concatenate((mainData,newData),axis=0)\n",
    "    return embed_con\n",
    "\n",
    "embed_trianX = concatenate_face(emb_trainX,emb_trainX_alex)\n",
    "embed_testX = concatenate_face(emb_testX,emb_testX_alex)\n",
    "trainy = concatenate_face(trainy_n,trainy_alex)\n",
    "testy = concatenate_face(testy_n,testy_alex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer , LabelEncoder\n",
    "\n",
    "in_coder = Normalizer()\n",
    "out_coder = LabelEncoder()\n",
    "\n",
    "embed_trainX_norm = in_coder.fit_transform(embed_trianX)\n",
    "embed_testX_norm = in_coder.transform(embed_testX)\n",
    "out_coder.fit(trainy)\n",
    "trainy_enc = out_coder.transform(trainy)\n",
    "testy_enc = out_coder.transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.000\n",
      "Test Accuracy: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         4\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       1.00      1.00      1.00         6\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      1.00      1.00         3\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         4\n",
      "          15       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        65\n",
      "   macro avg       1.00      1.00      1.00        65\n",
      "weighted avg       1.00      1.00      1.00        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score , classification_report\n",
    "\n",
    "model = SVC(probability=True,kernel='linear')\n",
    "model.fit(embed_trainX_norm,trainy_enc)\n",
    "\n",
    "#predict\n",
    "yhat_train = model.predict(embed_trainX_norm)\n",
    "yhat_test = model.predict(embed_testX_norm)\n",
    "\n",
    "print('Train Accuracy: {:.3f}'.format(accuracy_score(trainy_enc,yhat_train)))\n",
    "print('Test Accuracy: {:.3f}'.format(accuracy_score(testy_enc,yhat_test)))\n",
    "print(classification_report(testy_enc,yhat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "detector = MTCNN()\n",
    "facenet_model = FaceNet()\n",
    "\n",
    "\n",
    "def extract_face(filename, re_size=(160, 160)):\n",
    "    image_arr = np.array(filename)\n",
    "    faces = detector.detect_faces(image_arr)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        # No faces detected in the image\n",
    "        return None\n",
    "    \n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = image_arr[y1:y2, x1:x2]\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(re_size)\n",
    "    face_arr = np.asarray(image)\n",
    "    return face_arr\n",
    "\n",
    "def get_embedding(facenet_model, face):\n",
    "    if face is None:\n",
    "        return None\n",
    "    \n",
    "    face_pixel = face.astype('float32')\n",
    "    samples = np.expand_dims(face_pixel, axis=0)\n",
    "    embedd = facenet_model.embeddings(samples)\n",
    "    return embedd[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "16/16 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "['Alex']\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "3/3 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "['Alex']\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "['Alex']\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "['Alex']\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        face = extract_face(frame)\n",
    "        face_embed = get_embedding(facenet_model, face)\n",
    "        if face_embed is not None:\n",
    "            samples = np.expand_dims(face_embed,axis=0)\n",
    "            index = model.predict(samples)\n",
    "            predict_names = out_coder.inverse_transform(index)\n",
    "            print(predict_names)\n",
    "            cv2.imshow('output', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('d'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret , frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('output',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('d'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
